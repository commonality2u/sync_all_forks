# ======================================================================
# Workflow Name: Enhanced Fork Sync with Smart Batching
# ======================================================================
name: Sync Forks and Update README (Enhanced)

# ======================================================================
# Triggers: When the workflow runs
# ======================================================================
on:
  schedule:
    # Runs every 6 hours to spread load and catch updates faster
    - cron: '15 */6 * * *'
  workflow_dispatch:
    inputs:
      specific_repo:
        description: 'Optional: Specific repository (owner/repo) to sync'
        required: false
        type: string
      force_sync:
        description: 'Force sync (overwrite fork changes if conflicts)'
        required: false
        type: boolean
        default: false
      batch_size:
        description: 'Batch size for processing (default: 25)'
        required: false
        type: number
        default: 25
      max_parallel_jobs:
        description: 'Max parallel sync jobs (default: 5)'
        required: false
        type: number
        default: 5

# ======================================================================
# Permissions
# ======================================================================
permissions:
  contents: write
  actions: read

# ======================================================================
# Jobs
# ======================================================================
jobs:
  # ==============================================================
  # Job 1: List Forks and Check for Updates
  # ==============================================================
  analyze-forks:
    name: 1. Analyze Forks & Check Updates
    runs-on: ubuntu-latest
    timeout-minutes: 30
    outputs:
      forks_details_json: ${{ steps.analyze_forks.outputs.details_json }}
      forks_needing_sync_json: ${{ steps.analyze_forks.outputs.sync_needed_json }}
      total_forks_count: ${{ steps.analyze_forks.outputs.total_count }}
      sync_needed_count: ${{ steps.analyze_forks.outputs.sync_count }}
      batches_json: ${{ steps.create_batches.outputs.batches_json }}
      batch_count: ${{ steps.create_batches.outputs.batch_count }}
    env:
      GH_TOKEN: ${{ secrets.SYNC_TOKEN }}
      SPECIFIC_REPO: ${{ github.event.inputs.specific_repo }}
      GH_REPO_OWNER: ${{ github.repository_owner }}
    steps:
      - name: Check Dependencies
        run: |
          gh --version
          jq --version
          curl --version

      - name: Authenticate gh CLI
        run: gh auth setup-git

      - name: Analyze forks and check for updates
        id: analyze_forks
        run: |
          set -e
          
          echo "Starting fork analysis..."
          
          # Initialize variables
          all_forks_json="[]"
          forks_needing_sync="[]"
          total_count=0
          sync_needed=0
          
          # Define fields for comprehensive analysis
          JSON_FIELDS="nameWithOwner,description,parent,primaryLanguage,pushedAt,updatedAt,defaultBranchRef"
          
          if [[ -n "$SPECIFIC_REPO" ]]; then
            # Handle specific repo
            target_repo="$SPECIFIC_REPO"
            if [[ "$target_repo" != */* ]]; then
              target_repo="$GH_REPO_OWNER/$target_repo"
            fi
            
            echo "Analyzing specific repository: $target_repo"
            repo_details=$(gh repo view "$target_repo" --json $JSON_FIELDS 2>/dev/null || echo "null")
            
            if [[ "$repo_details" != "null" ]]; then
              all_forks_json=$(echo "$repo_details" | jq '[.]')
              forks_needing_sync="$all_forks_json"  # Always sync when specifically requested
              total_count=1
              sync_needed=1
            fi
          else
            # Handle all forks with pagination support
            echo "Fetching all forks for $GH_REPO_OWNER..."
            
            page=1
            per_page=100
            has_more=true
            
            while [[ "$has_more" == "true" ]]; do
              echo "Fetching page $page..."
              
              # Use gh api for better pagination control
              page_data=$(gh api "users/$GH_REPO_OWNER/repos" \
                --paginate \
                --jq '.[] | select(.fork == true) | {nameWithOwner: .full_name, description, parent: {nameWithOwner: .parent.full_name}, primaryLanguage: {name: .language}, pushedAt: .pushed_at, updatedAt: .updated_at, defaultBranchRef: {name: .default_branch}}' \
                --per-page $per_page \
                --page $page 2>/dev/null || echo "")
              
              if [[ -z "$page_data" ]]; then
                has_more=false
                break
              fi
              
              # Convert to JSON array and merge
              page_json=$(echo "$page_data" | jq -s '.')
              all_forks_json=$(echo "$all_forks_json $page_json" | jq -s 'add')
              
              page_count=$(echo "$page_json" | jq 'length')
              total_count=$((total_count + page_count))
              
              echo "Found $page_count forks on page $page (total so far: $total_count)"
              
              # Check if we got fewer than per_page results (last page)
              if [[ $page_count -lt $per_page ]]; then
                has_more=false
              fi
              
              page=$((page + 1))
              
              # Rate limiting protection
              sleep 1
            done
          fi
          
          echo "Total forks found: $total_count"
          
          # Now check which forks actually need syncing
          if [[ $total_count -gt 0 ]]; then
            echo "Checking which forks need syncing..."
            
            # Create a temporary file to store forks that need syncing
            temp_sync_file=$(mktemp)
            echo "[]" > "$temp_sync_file"
            
            echo "$all_forks_json" | jq -c '.[]' | while IFS= read -r fork; do
              fork_name=$(echo "$fork" | jq -r '.nameWithOwner')
              parent_name=$(echo "$fork" | jq -r '.parent.nameWithOwner // empty')
              
              if [[ -z "$parent_name" || "$parent_name" == "null" ]]; then
                echo "Skipping $fork_name - no parent found"
                continue
              fi
              
              echo "Checking sync status for $fork_name..."
              
              # Check if fork is behind upstream
              sync_status=$(gh repo sync "$fork_name" --dry-run 2>&1 || echo "needs_sync")
              
              if [[ "$sync_status" == *"already in sync"* ]]; then
                echo "$fork_name is up to date"
              else
                echo "$fork_name needs syncing"
                # Add to sync list
                current_list=$(cat "$temp_sync_file")
                updated_list=$(echo "$current_list" | jq --argjson fork "$fork" '. + [$fork]')
                echo "$updated_list" > "$temp_sync_file"
                sync_needed=$((sync_needed + 1))
              fi
              
              # Rate limiting
              sleep 0.5
            done
            
            forks_needing_sync=$(cat "$temp_sync_file")
            rm "$temp_sync_file"
          fi
          
          echo "Forks needing sync: $sync_needed out of $total_count"
          
          # Set outputs
          {
            echo "details_json<<EOF"
            echo "$all_forks_json"
            echo "EOF"
            echo "sync_needed_json<<EOF" 
            echo "$forks_needing_sync"
            echo "EOF"
            echo "total_count=$total_count"
            echo "sync_count=$sync_needed"
          } >> $GITHUB_OUTPUT

      - name: Create sync batches
        id: create_batches
        run: |
          sync_needed_json='${{ steps.analyze_forks.outputs.sync_needed_json }}'
          sync_count=${{ steps.analyze_forks.outputs.sync_count }}
          batch_size=${{ github.event.inputs.batch_size || 25 }}
          
          if [[ $sync_count -eq 0 ]]; then
            echo "No repos need syncing, creating empty batch"
            echo "batches_json=[]" >> $GITHUB_OUTPUT
            echo "batch_count=0" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "Creating batches of size $batch_size for $sync_count repositories..."
          
          # Create batches
          batches=$(echo "$sync_needed_json" | jq --argjson size "$batch_size" '
            [range(0; length; $size) as $i | .[$i:$i+$size]]
          ')
          
          batch_count=$(echo "$batches" | jq 'length')
          
          echo "Created $batch_count batches"
          
          {
            echo "batches_json<<EOF"
            echo "$batches"
            echo "EOF"
            echo "batch_count=$batch_count"
          } >> $GITHUB_OUTPUT

  # ==============================================================
  # Job 2: Generate README
  # ==============================================================
  generate-readme:
    name: 2. Generate README
    needs: analyze-forks
    if: needs.analyze-forks.outputs.total_forks_count > 0
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Generate enhanced README
        run: |
          JSON_DATA='${{ needs.analyze-forks.outputs.forks_details_json }}'
          TOTAL_COUNT=${{ needs.analyze-forks.outputs.total_forks_count }}
          SYNC_COUNT=${{ needs.analyze-forks.outputs.sync_needed_count }}
          GENERATION_DATE=$(date -u +"%Y-%m-%d %H:%M:%S UTC")
          
          # Create README with enhanced information
          cat > README.md << EOF
          # Synced Fork Repositories
          
          This repository manages **$TOTAL_COUNT** forked repositories using automated sync workflows.
          
          ## Status Overview
          - **Total Forks**: $TOTAL_COUNT
          - **Recently Synced**: $SYNC_COUNT repos needed updates in last run
          - **Last Updated**: $GENERATION_DATE
          - **Sync Frequency**: Every 6 hours
          
          ## Repository List
          
          | Repository | Upstream | Language | Description | Last Updated |
          |------------|----------|----------|-------------|--------------|
          EOF
          
          # Add table rows
          echo "$JSON_DATA" | jq -c '.[]' | while IFS= read -r repo; do
            name=$(echo "$repo" | jq -r '.nameWithOwner // "N/A"')
            parent=$(echo "$repo" | jq -r '.parent.nameWithOwner // "N/A"')
            language=$(echo "$repo" | jq -r '.primaryLanguage.name // "N/A"')
            description=$(echo "$repo" | jq -r '.description // ""' | sed 's/|/\\|/g')
            updated=$(echo "$repo" | jq -r '.updatedAt // "N/A"' | cut -d'T' -f1)
            
            repo_link="[$name](https://github.com/$name)"
            parent_link="N/A"
            if [[ "$parent" != "N/A" ]]; then
              parent_link="[$parent](https://github.com/$parent)"
            fi
            
            echo "| $repo_link | $parent_link | $language | $description | $updated |" >> README.md
          done
          
          # Add footer with workflow info
          cat >> README.md << EOF
          
          ## Automation Details
          
          This list is automatically maintained by the [Enhanced Fork Sync Workflow](.github/workflows/sync-forks.yml).
          
          - **Smart Sync**: Only syncs repositories that are actually behind their upstream
          - **Batch Processing**: Processes repositories in configurable batches to avoid rate limits
          - **Parallel Execution**: Runs multiple sync jobs in parallel for faster processing
          - **Failure Recovery**: Retries failed syncs with exponential backoff
          
          For manual sync or configuration, see the [Actions tab](../../actions).
          EOF

      - name: Commit and push README changes
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          git add README.md
          if git diff --staged --quiet; then
            echo "No changes to README.md"
            exit 0
          fi
          
          git commit -m "docs: Update fork repository list (${{ needs.analyze-forks.outputs.total_forks_count }} total, ${{ needs.analyze-forks.outputs.sync_needed_count }} synced)"
          
          # Retry logic for push
          for i in {1..3}; do
            if git push; then
              echo "Successfully pushed README update"
              break
            else
              echo "Push attempt $i failed, retrying..."
              git pull --rebase
              sleep 5
            fi
          done

  # ==============================================================
  # Job 3: Parallel Batch Sync Jobs
  # ==============================================================
  sync-batch:
    name: 3. Sync Batch
    needs: analyze-forks
    if: needs.analyze-forks.outputs.sync_needed_count > 0
    runs-on: ubuntu-latest
    timeout-minutes: 120
    strategy:
      max-parallel: ${{ fromJson(github.event.inputs.max_parallel_jobs || '5') }}
      fail-fast: false
      matrix:
        batch_index: ${{ fromJson(needs.analyze-forks.outputs.batch_count > 0 && format('[{0}]', join(fromJson(format('[{0}]', join(range(0, fromJson(needs.analyze-forks.outputs.batch_count)), ','))), ',')) || '[0]') }}
    env:
      GH_TOKEN: ${{ secrets.SYNC_TOKEN }}
      FORCE_SYNC: ${{ github.event.inputs.force_sync }}
    steps:
      - name: Setup batch processing
        id: setup
        run: |
          batch_index=${{ matrix.batch_index }}
          batches_json='${{ needs.analyze-forks.outputs.batches_json }}'
          
          # Extract current batch
          current_batch=$(echo "$batches_json" | jq --argjson idx "$batch_index" '.[$idx] // []')
          batch_size=$(echo "$current_batch" | jq 'length')
          
          echo "Processing batch $((batch_index + 1)) with $batch_size repositories"
          
          {
            echo "current_batch<<EOF"
            echo "$current_batch"
            echo "EOF"
            echo "batch_size=$batch_size"
          } >> $GITHUB_OUTPUT

      - name: Sync repositories in batch
        if: steps.setup.outputs.batch_size > 0
        run: |
          current_batch='${{ steps.setup.outputs.current_batch }}'
          batch_index=${{ matrix.batch_index }}
          
          echo "Starting sync for batch $((batch_index + 1))"
          
          successes=0
          failures=0
          declare -A failed_repos
          
          echo "$current_batch" | jq -c '.[]' | while IFS= read -r repo; do
            repo_name=$(echo "$repo" | jq -r '.nameWithOwner')
            
            echo "Syncing $repo_name..."
            
            # Build sync command
            sync_cmd="gh repo sync $repo_name"
            if [[ "$FORCE_SYNC" == "true" ]]; then
              sync_cmd="$sync_cmd --force"
            fi
            
            # Retry logic with exponential backoff
            max_attempts=3
            attempt=1
            delay=5
            
            while [[ $attempt -le $max_attempts ]]; do
              echo "Attempt $attempt for $repo_name"
              
              if output=$($sync_cmd 2>&1); then
                echo "âœ… Successfully synced $repo_name"
                echo "$output"
                successes=$((successes + 1))
                break
              else
                echo "âŒ Attempt $attempt failed for $repo_name: $output"
                
                if [[ $attempt -eq $max_attempts ]]; then
                  echo "::error::Failed to sync $repo_name after $max_attempts attempts"
                  failed_repos["$repo_name"]="$output"
                  failures=$((failures + 1))
                else
                  echo "Waiting ${delay}s before retry..."
                  sleep $delay
                  delay=$((delay * 2))
                fi
              fi
              
              attempt=$((attempt + 1))
            done
            
            # Rate limiting between repos
            sleep 2
          done
          
          echo "Batch $((batch_index + 1)) complete: $successes successes, $failures failures"
          
          # Report to step summary
          echo "## Batch $((batch_index + 1)) Results" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Successful: $successes" >> $GITHUB_STEP_SUMMARY
          echo "- âŒ Failed: $failures" >> $GITHUB_STEP_SUMMARY
          
          if [[ $failures -gt 0 ]]; then
            echo "### Failed Repositories:" >> $GITHUB_STEP_SUMMARY
            for repo in "${!failed_repos[@]}"; do
              echo "- **$repo**: ${failed_repos[$repo]}" >> $GITHUB_STEP_SUMMARY
            done
          fi

  # ==============================================================
  # Job 4: Final Summary
  # ==============================================================
  workflow-summary:
    name: 4. Workflow Summary
    needs: [analyze-forks, generate-readme, sync-batch]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Generate final summary
        run: |
          echo "# ðŸ”„ Fork Sync Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Run**: [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger**: \`${{ github.event_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Date**: $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Repository statistics
          echo "## ðŸ“Š Repository Statistics" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Forks**: ${{ needs.analyze-forks.outputs.total_forks_count || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Needed Sync**: ${{ needs.analyze-forks.outputs.sync_needed_count || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Batches Created**: ${{ needs.analyze-forks.outputs.batch_count || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Job status summary
          echo "## ðŸŽ¯ Job Status Summary" >> $GITHUB_STEP_SUMMARY
          
          analyze_status="${{ needs.analyze-forks.result }}"
          readme_status="${{ needs.generate-readme.result }}"
          sync_status="${{ needs.sync-batch.result }}"
          
          case "$analyze_status" in
            success) echo "- âœ… **Analysis**: Completed successfully" >> $GITHUB_STEP_SUMMARY ;;
            failure) echo "- âŒ **Analysis**: Failed" >> $GITHUB_STEP_SUMMARY ;;
            *) echo "- â¸ï¸ **Analysis**: $analyze_status" >> $GITHUB_STEP_SUMMARY ;;
          esac
          
          case "$readme_status" in
            success) echo "- âœ… **README Update**: Completed successfully" >> $GITHUB_STEP_SUMMARY ;;
            failure) echo "- âŒ **README Update**: Failed" >> $GITHUB_STEP_SUMMARY ;;
            skipped) echo "- â­ï¸ **README Update**: Skipped (no changes needed)" >> $GITHUB_STEP_SUMMARY ;;
            *) echo "- â¸ï¸ **README Update**: $readme_status" >> $GITHUB_STEP_SUMMARY ;;
          esac
          
          case "$sync_status" in
            success) echo "- âœ… **Repository Sync**: All batches completed successfully" >> $GITHUB_STEP_SUMMARY ;;
            failure) echo "- âš ï¸ **Repository Sync**: Some repositories failed to sync" >> $GITHUB_STEP_SUMMARY ;;
            skipped) echo "- â­ï¸ **Repository Sync**: Skipped (no repositories needed syncing)" >> $GITHUB_STEP_SUMMARY ;;
            *) echo "- â¸ï¸ **Repository Sync**: $sync_status" >> $GITHUB_STEP_SUMMARY ;;
          esac
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ’¡ **Tip**: Check individual job logs for detailed information about any failures." >> $GITHUB_STEP_SUMMARY
