# Workflow Name: Appears in the Actions tab
name: Sync Forks and Update README (Batched)

# Triggers: When the workflow runs
on:
  schedule:
    # Runs every 12 hours (e.g., at 00:15 and 12:15 UTC)
    - cron: '15 */12 * * *'
  workflow_dispatch:
    # Allows manual triggering via the Actions tab
    inputs:
      specific_repo:
        description: 'Specific repository (owner/repo) to sync (leave empty for all forks)'
        required: false
        type: string
      force_sync:
        description: 'Force sync (overwrite fork changes if conflicts) - USE WITH CAUTION!'
        required: false
        type: boolean
        default: false

# Permissions: Define default permissions for the GITHUB_TOKEN for all jobs
permissions:
  contents: write

jobs:
  # ==============================================================
  # Job 1: List Forks and Gather Details
  # ==============================================================
 # In jobs.list-forks:
# ... (env block as before) ...
steps:
  - name: Check gh CLI installation
    run: gh --version
  - name: Authenticate gh CLI for Git operations
    run: gh auth setup-git
  - name: Get list of forks with details
    id: get_forks
    run: |
      # Add trace mode if further debugging is needed: set -x
      repo_details_json="[]"
      repo_names_json="[]"
      repo_count=0
      JSON_FIELDS="nameWithOwner,description,parent,primaryLanguage"

      if [[ -n "$SPECIFIC_REPO" ]]; then
        target_repo="$SPECIFIC_REPO"
        echo "Fetching details for specific repository: $target_repo"
        if [[ "$target_repo" != */* ]]; then target_repo="$GH_REPO_OWNER/$target_repo"; echo "Assuming owner: $target_repo"; fi
        # Ensure output is compact JSON (no newlines within the JSON structure itself)
        fetched_details=$(gh repo view "$target_repo" --json $JSON_FIELDS | jq --compact-output '.')
        if [[ -n "$fetched_details" && "$fetched_details" != "null" ]]; then
           repo_details_json=$(echo "$fetched_details" | jq --compact-output '[.]') # Wrap in array
           repo_names_json=$(echo "$repo_details_json" | jq --compact-output '[.[].nameWithOwner]')
           repo_count=1
        else
           echo "::error::Failed to fetch details for specific repo: $target_repo. Check name and permissions."
        fi
      else
        echo "Fetching all fork repositories for $GH_REPO_OWNER with details..."
        # Ensure output is compact JSON
        repo_details_json=$(gh repo list "$GH_REPO_OWNER" --fork --limit 2000 --json $JSON_FIELDS | jq --compact-output '.')
        repo_count=$(echo "$repo_details_json" | jq 'length')
        repo_names_json=$(echo "$repo_details_json" | jq --compact-output '[.[].nameWithOwner]')
        echo "Found $repo_count forks."
      fi

      if [[ "$repo_count" -eq 0 ]]; then echo "::warning::No repositories found to process."; fi

      # --- Set Outputs using multi-line delimiter format ---
      # This handles JSON strings potentially spanning multiple lines when pretty-printed,
      # although --compact-output should minimize this. It's safer anyway.
      echo "details_json<<EOF" >> $GITHUB_OUTPUT
      echo "$repo_details_json" >> $GITHUB_OUTPUT
      echo "EOF" >> $GITHUB_OUTPUT

      echo "names_json<<EOF" >> $GITHUB_OUTPUT
      echo "$repo_names_json" >> $GITHUB_OUTPUT
      echo "EOF" >> $GITHUB_OUTPUT

      # Count is single line, standard format is fine
      echo "count=$repo_count" >> $GITHUB_OUTPUT

      echo "Outputs generated." # Add confirmation log


  # ==============================================================
  # Job 2: Generate README.md (Check detailed logs for Exit Code 2)
  # ==============================================================
  generate-readme:
    name: 2. Generate README File
    needs: list-forks
    if: needs.list-forks.outputs.forks_count > 0
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout Workflow Repo
        uses: actions/checkout@v4
        # Uses default GITHUB_TOKEN implicitly

      - name: Generate README content with detailed table
        id: generate
        run: |
          # Add trace mode for debugging if needed: set -x
          echo "Generating README.md content..."
          # Use the JSON with full details from list-forks
          JSON_DATA='${{ needs.list-forks.outputs.forks_details_json }}'
          REPO_COUNT=${{ needs.list-forks.outputs.forks_count }}
          WORKFLOW_FILE_PATH=".github/workflows/${{ github.workflow }}"
          GENERATION_DATE=$(date -u +"%Y-%m-%d %H:%M:%S UTC")

          TEMP_README=$(mktemp)
          echo "# Synced Fork Repositories" > "$TEMP_README"
          echo "" >> "$TEMP_README"
          echo "This file lists the **$REPO_COUNT** forked repositories managed by the [$WORKFLOW_FILE_PATH]($WORKFLOW_FILE_PATH) workflow." >> "$TEMP_README"
          echo "_Last updated: $GENERATION_DATE_" >> "$TEMP_README"
          echo "" >> "$TEMP_README"
          echo "| Repository | Upstream | Language | Description |" >> "$TEMP_README"
          echo "|------------|----------|----------|-------------|" >> "$TEMP_README"

          echo "$JSON_DATA" | jq -c '.[]' | while IFS= read -r repo_obj; do
            repo_name=$(echo "$repo_obj" | jq -r '.nameWithOwner // "N/A"')
            repo_link="[$repo_name](https://github.com/$repo_name)"
            upstream_name=$(echo "$repo_obj" | jq -r '.parent.nameWithOwner // "N/A"')
            upstream_link="N/A"; if [[ "$upstream_name" != "N/A" && "$upstream_name" != "null" ]]; then upstream_link="[$upstream_name](https://github.com/$upstream_name)"; fi
            language=$(echo "$repo_obj" | jq -r '.primaryLanguage.name // "N/A"'); if [[ "$language" == "null" ]]; then language="N/A"; fi
            description=$(echo "$repo_obj" | jq -r '.description // ""' | sed 's/|/\\|/g')
            echo "| $repo_link | $upstream_link | $language | $description |" >> "$TEMP_README"
          done
          mv "$TEMP_README" README.md
          echo "README.md generated."

      - name: Commit and Push README if changed
        run: |
          # Add trace mode for debugging if needed: set -x
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add README.md
          if git diff --staged --quiet; then
            echo "No changes detected in README.md. Skipping commit."
          else
            echo "Changes detected in README.md. Committing..."
            git commit -m "docs: Update list of synced forks (${{ needs.list-forks.outputs.forks_count }} repos)" -m "Workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            echo "Pushing changes..."
            attempt=0; max_attempts=3; push_success=false
            while [ $attempt -lt $max_attempts ]; do
              attempt=$((attempt + 1))
              if git push; then echo "Push successful on attempt $attempt."; push_success=true; break;
              else
                 echo "::warning::Push failed on attempt $attempt. Retrying in 15 seconds..."
                 sleep 15
                 echo "Pulling latest changes (rebase strategy)..."
                 # Important: Use --ff-only pull or handle potential rebase conflicts if needed.
                 # Simple pull might be safer if rebase causes issues. Let's try simple pull first.
                 if ! git pull --ff-only; then # Try fast-forward only first
                    echo "::warning::Fast-forward pull failed, attempting standard pull..."
                    if ! git pull; then # If ff-only fails, try standard merge pull
                       echo "::error::Pull failed. Cannot resolve conflicts automatically."
                       # Decide if you want to fail here or attempt push anyway
                       # exit 1 # Option: Fail the job if pull fails
                    fi
                 fi
              fi
            done
            if [ "$push_success" = false ]; then echo "::error::Failed to push README.md changes after $max_attempts attempts. Check logs and repository permissions/rules."; exit 1; fi
          fi

  # ==============================================================
  # Job 3: Sync Forks in Batches (Handles > 256 forks)
  # ==============================================================
  sync-batches:
    name: 3. Sync Batches
    needs: list-forks
    if: needs.list-forks.outputs.forks_count > 0
    runs-on: ubuntu-latest
    # Strategy: Generate batch numbers dynamically
    strategy:
      fail-fast: false # Don't stop other batches if one fails
      matrix:
        # This will be populated by the output of the 'generate_batches' step below
        batch_index: [0] # Dummy value, will be replaced
    outputs:
      # Optional: Could output lists of successes/failures per batch
      batch_result: ${{ steps.sync_batch_step.outputs.result }}

    steps:
      - name: Generate Batch Matrix
        id: generate_batches
        run: |
          COUNT=${{ needs.list-forks.outputs.forks_count }}
          BATCH_SIZE=50 # Adjust batch size as needed (50 * 12 batches = 600, well under 256 limit)
          NUM_BATCHES=$(( (COUNT + BATCH_SIZE - 1) / BATCH_SIZE )) # Ceiling division
          echo "Total Repos: $COUNT, Batch Size: $BATCH_SIZE, Num Batches: $NUM_BATCHES"
          if [[ $NUM_BATCHES -eq 0 ]]; then
            # Handle case with 0 repos if needed, though 'if' on job handles this
             echo "matrix_config={\"include\":[]}" >> $GITHUB_OUTPUT
          else
            # Generate sequence [0, 1, ..., NUM_BATCHES-1] as JSON for the matrix input
            MATRIX_JSON=$(jq -c -n --argjson max "$NUM_BATCHES" '[range(0;$max)]')
            echo "matrix_config={\"batch_index\":$MATRIX_JSON}" >> $GITHUB_OUTPUT
            # Example output: matrix_config={"batch_index":[0,1,2,3,4,5,6,7,8,9,10,11]}
          fi
      # The actual matrix strategy uses the output from the previous step
      # This requires a bit of a workaround as strategy is evaluated early.
      # We define a dummy matrix above and filter/run logic inside steps based on the calculated batches.
      # A cleaner way requires more complex dynamic matrix generation often involving separate actions/workflows.
      # Let's stick to the slicing logic *within* the job based on an index derived from the total count.
      # --> Reverting to a simpler matrix generation if possible, or slicing within the job.

      # --- Simpler Approach: Slice Within Job ---
      # The strategy above is complex. Let's redefine the job to run ONCE,
      # and calculate batches internally. This sacrifices parallelism between batches
      # but avoids matrix complexity and the 256 limit directly.
      # If parallelism between batches IS required, the dynamic matrix is needed.
      # Let's assume sequential batch processing within ONE job is acceptable first.

# =====================================================================
# == REVISED Job 3: Process All Batches Sequentially in One Job ==
# =====================================================================
  sync_all_forks_sequentially_batched:
    name: 3. Sync All Forks (Batched Sequential)
    needs: list-forks
    if: needs.list-forks.outputs.forks_count > 0
    runs-on: ubuntu-latest
    # Increase timeout significantly as this job processes ALL forks
    timeout-minutes: 300 # 5 hours, adjust as needed up to 360 (6 hours)
    env:
      GH_TOKEN: ${{ secrets.SYNC_TOKEN }}
      FORCE_SYNC: ${{ github.event.inputs.force_sync }}
    steps:
      - name: Check gh CLI installation
        run: gh --version
      - name: Configure Git User (Optional)
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
      - name: Authenticate gh CLI for Git operations
        run: gh auth setup-git

      - name: Sync All Forks in Batches
        id: sync_step
        run: |
          # Add trace mode for debugging if needed: set -x
          JSON_NAMES='${{ needs.list-forks.outputs.forks_names_json }}'
          COUNT=${{ needs.list-forks.outputs.forks_count }}
          BATCH_SIZE=50 # Process 50 forks at a time
          FAILURES=0
          SUCCESSES=0
          FAILED_REPOS=() # Array to hold names of failed repos

          echo "Starting sequential sync for $COUNT repositories in batches of $BATCH_SIZE..."

          for (( batch_start=0; batch_start<COUNT; batch_start+=BATCH_SIZE )); do
            batch_end=$((batch_start + BATCH_SIZE))
            # Ensure batch_end doesn't exceed total count
            if (( batch_end > COUNT )); then batch_end=$COUNT; fi

            echo "--- Processing Batch: Repos ${batch_start} to $((batch_end - 1)) ---"

            # Extract the names for the current batch
            BATCH_NAMES_JSON=$(echo "$JSON_NAMES" | jq --argjson start "$batch_start" --argjson end "$batch_end" '.[$start:$end]')

            # Loop through repos in the current batch
            echo "$BATCH_NAMES_JSON" | jq -r '.[]' | while IFS= read -r repo_name; do
              if [[ -z "$repo_name" || "$repo_name" == "null" ]]; then continue; fi # Skip empty names

              echo "--> Syncing $repo_name..."
              attempt=0; max_attempts=3; delay=10; sync_success=false
              sync_command="gh repo sync $repo_name"; if [[ "${{ env.FORCE_SYNC }}" == "true" ]]; then sync_command="$sync_command --force"; fi

              while [ $attempt -lt $max_attempts ]; do
                attempt=$((attempt + 1))
                output=$( { $sync_command; } 2>&1 ); status=$?
                if [ $status -eq 0 ]; then sync_success=true; echo "Sync successful for $repo_name."; echo "$output"; break; fi
                echo "::warning::Sync attempt $attempt failed for $repo_name (Exit Code: $status). Output: $output"
                if [ $attempt -ge $max_attempts ]; then echo "::error::Failed to sync $repo_name after $max_attempts attempts."; break; fi
                echo "Waiting $delay seconds before retry..."; sleep $delay; delay=$((delay * 2))
              done # End retry loop

              if [ "$sync_success" = true ]; then
                SUCCESSES=$((SUCCESSES + 1))
                echo "✅ Successfully processed $repo_name" >> $GITHUB_STEP_SUMMARY
              else
                FAILURES=$((FAILURES + 1))
                FAILED_REPOS+=("$repo_name") # Add to list of failures
                echo "❌ Failed to sync $repo_name" >> $GITHUB_STEP_SUMMARY
                 # Optionally add failure log snippet to summary here too
              fi
              echo "-----------------------------------------"
              # Optional: Add a small delay between repos within a batch
              # sleep 1
            done # End repo loop for batch
            echo "--- Finished Batch: Repos ${batch_start} to $((batch_end - 1)) ---"
             # Optional: Add a small delay between batches
             # sleep 5
          done # End batch loop

          echo "--- Sync Process Complete ---"
          echo "Total Repos Processed: $COUNT"
          echo "Successful Syncs: $SUCCESSES"
          echo "Failed Syncs: $FAILURES"
          echo "$SUCCESSES" >> $GITHUB_OUTPUT # Example output
          echo "$FAILURES" >> $GITHUB_OUTPUT # Example output

          # Update Job Summary with failure list
          if [[ $FAILURES -gt 0 ]]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Failed Repositories ($FAILURES):" >> $GITHUB_STEP_SUMMARY
            printf -- '- %s\n' "${FAILED_REPOS[@]}" >> $GITHUB_STEP_SUMMARY
            # Fail the step if any repo failed
            exit 1
          fi
          # If no failures, exit successfully
          exit 0

  # ==============================================================
  # Job 4: Final Summary Job
  # ==============================================================
  sync-summary:
    name: 4. Workflow Summary
    # Depends on list-forks, generate-readme, and the new sync job
    needs: [list-forks, generate-readme, sync_all_forks_sequentially_batched]
    runs-on: ubuntu-latest
    timeout-minutes: 5
    # Run even if preceding jobs failed/were skipped
    if: always()
    steps:
      - name: Report Overall Status
        run: |
          echo "## Fork Sync & README Update Summary" >> $GITHUB_STEP_SUMMARY
          echo "- Workflow Run ID: ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- Triggered By: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- Total Forks Found: ${{ needs.list-forks.outputs.forks_count }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # --- Report on README Job ---
          readme_outcome="${{ needs.generate-readme.result }}"
          readme_status_msg="-"; if [[ "${{ needs.list-forks.outputs.forks_count }}" -gt 0 ]]; then case "$readme_outcome" in (success) readme_status_msg="✅ README update job completed successfully.";; (skipped) readme_status_msg="ℹ️ README update job skipped.";; (failure) readme_status_msg="⚠️ README update job failed. Check its logs.";; (cancelled) readme_status_msg="ℹ️ README update job cancelled.";; (*) readme_status_msg="ℹ️ README update job status: ${readme_outcome}.";; esac; else readme_status_msg="ℹ️ README update job skipped (no forks found)."; fi; echo "$readme_status_msg" >> $GITHUB_STEP_SUMMARY

          # --- Report on Sync Job ---
          sync_outcome="${{ needs.sync_all_forks_sequentially_batched.result }}"
          sync_status_msg="-"; if [[ "${{ needs.list-forks.outputs.forks_count }}" -gt 0 ]]; then case "$sync_outcome" in (success) sync_status_msg="✅ Sync job completed successfully (all forks processed without final error).";; (failure) sync_status_msg="⚠️ Sync job failed (one or more forks failed to sync). Check logs & summary.";; (cancelled) sync_status_msg="ℹ️ Sync job cancelled.";; (skipped) sync_status_msg="ℹ️ Sync job skipped.";; (*) sync_status_msg="ℹ️ Sync job status: ${sync_outcome}.";; esac; else sync_status_msg="ℹ️ Sync job skipped (no forks found)."; fi; echo "$sync_status_msg" >> $GITHUB_STEP_SUMMARY

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "*Check the 'Sync All Forks (Batched Sequential)' job logs and summary panel for per-repository status and failures.*" >> $GITHUB_STEP_SUMMARY
